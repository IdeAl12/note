[TOC]

# 论文阅读

## Learning Dexterous In-Hand Manipulation

我们使用强化学习来学习灵巧的手持操作策略，这些策略可以在shadow灵巧手上执行基于视觉的对象重定向。 训练是在模拟环境中进行的，在该模拟环境中，我们**将系统的许多物理属性随机化**，如摩擦系数和物体外观。 尽管我们完全接受了模拟训练，但我们的政策仍转移到物理机器。我们的方法<u>不依赖于任何人类示范，但是在人类操纵中发现的许多行为自然地出现</u>，包括手指滑动，多指协调以及重力的受控使用。 我们的结果是使用用于训练OpenAI Five的相同分布式RL系统获得的[43]。

### Introduction

虽然对物体的灵巧操纵是人类的基本日常任务，但对自主机器人来说仍然具有挑战性。 <u>现代机器人通常设计用于受限设置中的特定任务，并且很大程度上无法利用复杂的末端效应器。</u> 相比之下，人们能够在各种环境中执行各种灵巧的操作任务，使人手成为机器人操纵研究的灵感源泉。

Shadow灵巧手[58]是为人类灵敏度设计的机器人手的一个例子; 它有五个手指，共有24个自由度。该手自2005年以来已经商业化;然而，它仍未被广泛采用，<u>这可归因于控制这种复杂系统的艰巨困难</u>。控制五指手的最先进技术受到严重限制。一些现有方法已经在模拟中显示了有希望的手动操作结果，但是没有尝试转移到现实世界的机器人[5,40]。相反，由于难以对这种复杂系统进行建模，因此还有一些仅在物理机器人上进行训练的方法[16,67,29,30]。然而，由于物理试验是如此缓慢且成本高昂，因此学习行为非常有限。

在这项工作中，我们演示了训练控制策略的方法，这些控制策略执行手动操作并将其部署在物理机器人上。 由此产生的政策表现出前所未有的灵巧性，并自然发现人类中的抓握类型，如三脚架，棱柱形和尖端捏抓，并显示接触丰富的动态行为，如手指滑动，多指协调，受控使用 重力，并协调应用平移和扭转力到物体。 我们的政策还可以使用视觉来感知物体的姿势 - 机器人的一个重要方面应该最终在受控制的实验室环境之外工作。

尽管完全在与现实世界大不相同的模拟器中进行训练，但我们获得了在物理机器人上表现良好的控制策略。我们将转移结果归因于<u>（1）模拟环境中的广泛随机化和附加效应以及校准，（2）记忆增强控制策略，其允许在运行中学习自适应行为和隐式系统识别的可能性，以及（3）在大规模分布式强化学习。</u>我们的方法概述如图2所示。

![屏幕快照 2018-09-26 下午7.56.26](/Users/lixiang/Documents/Baxter/assets/屏幕快照 2018-09-26 下午7.56.26.png)

图2：系统概述。 （a）我们<u>使用具有随机参数和外观的大量模拟分布来收集控制策略和基于视觉的姿势估计器的数据。</u> （b）控制策略接收来自分布式模拟的观察到的机器人状态和奖励，并学习使用循环神经网络和强化学习将观察结果映射到动作。 （c）基于视觉的姿势估计器呈现从分布式模拟收集的场景并且学习使用与控制策略分开训练的卷积神经网络（CNN）从图像预测对象的姿势。 （d）为了转移到现实世界，我们使用<u>CNN预测来自3个真实摄像机馈送的物体姿势，使用3D运动捕捉系统测量机器人指尖位置，并将这两者用于控制策略以产生机器人的动作。</u>

该论文的结构如下。第2节给出了系统概述，更详细地描述了建议的任务，并显示了硬件设置。第3节描述了对控制策略，环境随机化以及添加到模拟器的附加效果的观察，这些效果使得传输成为可能。第4节概述了控制策略培训程序和分布式RL系统。第5节描述了视觉模型架构和培训程序。最后，第6节描述了在物理机器人上部署控制策略和视觉模型的定性和定量结果。

### Task and System Overview

在这项工作中，我们考虑手头物体重新定位的问题。我们将对象放在人形机器人手掌上。目标是将对象重新定向到所需的目标配置。一旦达到（近似）当前目标，就提供新目标直到最终丢弃对象。我们使用两个不同的对象，一个块和一个八角形棱镜。图3描绘了我们的物理系统以及我们的模拟环境。
#### Hardware
我们使用Shadow灵巧手，这是<u>一种具有24个自由度（DoF）的人形机器人手</u>，由20对激动剂 - 拮抗剂肌腱驱动。我们使用PhaseSpace运动捕捉系统来跟踪所有五个指尖的笛卡尔位置。对于对象姿势，我们有两个设置：<u>一个使用PhaseSpace标记来跟踪对象，另一个使用三个Basler RGB相机进行基于视觉的姿势估计</u>。这是因为我们的目标是最终拥有一个在实验室环境之外工作的系统，基于视觉的系统能够更好地应对现实世界。我们不使用手中嵌入的触摸传感器，仅使用关节感应来实现低级相对位置控制。我们更新低电平控制器的目标，该控制器大约运行1 kHz，控制策略给出的相对位置大约为12 Hz。
有关我们硬件设置的更多详细信息，请参见附录B.

#### Simulation

我们<u>使用MuJoCo物理引擎模拟物理系统[64]，我们使用Unity2渲染图像以训练基于视觉的姿势估计器</u>。 我们的Shadow Dexterous Hand模型基于OpenAI Gym机器人环境[49]中使用的模型，但经过校准后已经过改进以更紧密地匹配物理系统（详见附录C.3）。

尽管我们进行了校准工作，但模拟仍然是物理设置的粗略近似。 例如，我们的模型直接将扭矩应用于关节而不是基于肌腱的致动，并使用刚体接触模型而不是可变形的身体接触模型。 在刚体模拟器中对在现实世界中看到的这些和其他效果进行建模是困难的或不可能的。 这些差异导致了“现实差距”，并且使得在模拟中训练有这些不准确性的政策不太可能很好地转移。
我们在附录C.1中描述了我们模拟的其他细节。

### Transferable Simulations

我们的模拟是对现实世界的粗略近似。 因此，我们面临两难选择：我们无法对物理机器人进行训练，因为深层强化学习算法需要数百万个样本; 相反，仅在模拟中进行培训会导致由于模拟环境和真实环境之间的差距而无法很好地转移的策略。 <u>为了克服现实差距，我们将模拟的基本版本修改为许多模拟的分布，以促进转移[54,62,45]。</u> 通过仔细选择传感模式并通过随机化模拟环境的大多数方面，我们能够训练不太可能过度拟合到特定模拟环境的策略，并且更有可能成功传输到物理机器人。

#### Observation

我们<u>使用PhaseSpace标记给出指尖的控制策略观察，并使用PhaseSpace标记或基于视觉的姿势估计器提供对象姿势。</u>虽然暗影灵巧之手包含大量内置传感器，但我们特别避免将这些作为对策略的观察结果，因为它们受到依赖于状态的噪声的影响，这些噪声很难在模拟器中建模。例如，指尖触觉传感器测量存储在指尖内的气囊中的流体的压力，其与施加到指尖的力相关，但也与许多混杂变量相关，包括大气压力，温度和形状。接触和交叉几何。尽管在模拟器中确定接触的存在是直截了当的，但是很难对传感器值的分布进行建模。类似的考虑适用于由霍尔效应传感器测量的关节角度，霍尔效应传感器由低级控制器使用但由于它们倾向于噪声且难以校准而未提供给策略。

#### Randomizations

在之前关于领域随机化[54,62,45]的研究之后，我们将模拟环境的大部分方面随机化，以便学习一种概括为现实的政策和视觉模型。 我们简要介绍下面的随机化类型，附录C.2包含更多涉及更多随机化的详细讨论并提供超参数。

**观察噪音。** 为了更好地模拟我们期望在现实中体验的噪声，我们<u>在策略观察中添加高斯噪声</u>。 特别是，我们应用相关噪声，每次采样一次采样，以及在每个时间步采样的不相关噪声。

**物理随机化**。像摩擦这样的物理参数<u>在每集开始时随机化并保持固定</u>。许多参数以模型校准期间发现的值为中心，以使模拟分布更紧密地匹配现实。表1列出了随机化的所有物理参数。

**未建模的效果**。物理机器人经历了许多未通过我们的模拟建模的效果。为了解释不完美的驱动，我们<u>使用简单的电机间隙模型</u>，并在将其应用于模拟之前引入动作延迟和动作噪声。我们的运动捕捉设置有时会暂时失去对标记的跟踪，我们通过在模拟中短时间内以低概率冻结模拟标记的位置来建模。我们还通过<u>在接近另一个标记或对象时冻结其模拟位置来模拟标记物遮挡</u>。<u>为了处理额外的未建模动态，我们对对象应用小的随机力。</u>有关具体实施的详细信息，请参见附录C.2。

**视觉外观随机化**。我们随机化渲染场景的以下方面：<u>相机位置和内在，照明条件，手和物体的姿势，以及场景中所有对象的材质和纹理</u>。图4描绘了这些随机环境的一些示例。有关随机特性及其范围的详细信息，请参见附录C.2。

### Learning Control Policies From State

#### Policy Arvhitecture

我们使用的许多随机化在一集中持续存在，因此<u>内存增强策略应该可以识别当前环境的属性并相应地调整其自身行为。</u>例如，与环境交互的初始步骤可以揭示对象的重量或食指可以移动多快。因此，我们将策略表示为具有存储器的递归神经网络，即LSTM [25]，其具有在输入和LSTM之间插入ReLU [41]激活的附加隐藏层。

该政策采用<u>近端政策优化（PPO</u>）[57]进行培训。我们在附录A中更详细地提供了强化学习和PPO的背景.PPO需要培训两个网络 - 一个将观察结果映射到行动的政策网络，以及一个预测从给定开始的未来奖励的折扣总和的价值网络。州。两个网络都具有相同的架构，但具有独立的参数。<u>由于价值网络仅在训练期间使用，我们使用非对称Actor-Critic [46]方法</u>。非对称Actor-Critic利用价值网络可以访问真实机器人系统上无法获得的信息这一事实.这可能会简化学习良好价值估计的问题，因为需要推断的信息较少。表2列出了馈送到两个网络的输入列表。