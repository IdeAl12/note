# 强化学习的未来

## 免模型方法的缺陷

1. 免模型方法无法从不带反馈信号的样本中学习，二反馈本身是稀疏的，因此免模型方法样本利用率很低，而数据驱动的方法需要大量采样，而但凡与机械控制相关的问题，训练数据远不如视频图像的数据容易获取，因此只能在模拟器中训练。但是模拟器和现实世界之间的reality gap，直接限制了算法的泛化性能。数据的稀缺性也影响了其与DL技术的结合。
2. 免模型方法不对具体问题进行建模，而是尝试用一个通用的算法解决所有问题。基于模型的方法针对特定问题建立模型，充分利用了问题固有的信息。免模型方法在追求通用性的同时放弃这些富有价值的信息。
3. 基于模型的方法针对问题建立动力学模型，具有解释性。而免模型方法因为没有模型，解释性不强，调试困难。
4. 相比基于模型的方法，尤其是基于简单线性模型的方法，免模型方法不够稳定，训练过程中容易发散。

## 为什么DRL的工作基于免模型方法

1. 免模型的方法相对简单直观，开源实现丰富，比较容易上手。
2. 当前RL的发展还处于初级阶段，研究的重点还是集中在环境是确定的、静态的，状态主要是离散的、静态的、完全可观察的，反馈也是确定的问题。
3. 高估了DRL的能力。DQN展示出的令人兴奋的能力使得很多人围绕着DQN进行拓展，创造出了一系列同样属于免模型的工作。

## 基于模型的方法的优点

基于模型的方法一般先从数据中学习模型，然后基于学到的模型对策略进行优化。因为模型的存在，基于模型的方法可以充分利用每一个样本来逼近模型，数据利用率极大提高。此外，学到的模型汪汪队环境的变化鲁邦，当遇到新环境时，算法可以依靠已学到的模型做推理，具有很好的泛化性。

基于模型的方法还与潜力巨大的预测学习（Predictive Learning）紧密相关。

## 基于模型的方法的缺点

1. 基于模型的DRL方法相对而言不那么简单直观，RL与DL的结合方式相对更复杂，设计难度更高。目前基于模型的DRL方法通常用高斯过程、贝叶斯网络或概率神经网络（PNN）来构建模型，如Predictron模型，Probabilistic Inference for Learning COntrol (PILCO)模型或Guided Policy Search (GPS)模型。
2. 针对无法建模的问题束手无策，有些领域，比如Natural Language Processing（NLP），存在大量难以归纳成模型的任务。
3. 建模会带来误差，而且误差往往随着算法与环境的迭代交互越来越大，使得算法难以保证收敛到最优解。
4.  模型缺乏通用性，每次换一个问题，就要重新建模。

- 半模型方法：RL泰斗Rich Sutton提出的Dyna框架和其弟子David Silver提出的Dyna-2框架

## 重新审视强化学习

目前DRL在解决部分可见状态任务（如StarCraft），状态连续的任务（如机械控制任务），动态反馈任务和多代理任务中还没取得令人惊叹的突破。

当前大量的DRL研究，尤其是应用于计算机视觉领域任务的研究中，很多都是将计算机视觉的某一个基于DL的任务强行构造成RL问题进行求解，其结果往往不如传统方法好。这样的研究方式造成DRL领域论文数量暴增、水分巨大。研究者不应该找一个DL任务强行将其RL化，而是应该针对一些天然适合RL处理的任务，尝试通过引入DL来提升现有方法在目标识别环节或函数逼近环节上的能力。

在计算机视觉任务中，通过结合DL获得良好的特征表达或函数逼近是非常自然的思路。但在有些领域，DL未必能发挥强大的特征提取作用，也未必被用于函数逼近。比如DL至今在机器人领域最多起到感知作用，而无法取代基于力学分析的方法。

DRL算法的应用特点：因为其输出的随机性，当前的DRL算法更多地被用在模拟器而非真实环境中。如果能针对某一具体问题，解决模拟器与真实世界间的差异，则可以发挥DRL的强大威力。

不过，考虑到RL算法的不稳定性，在实际应用中不应盲目追求端到端的解决方案，而可以考虑将特征提取（DL）与决策（RL）分开，从而获得更好的解释性与稳定性。

此外，模块化RL（将RL算法封装成一个模块）以及将RL与其他模型融合，将在实际应用中有广阔前景。而如何通过DL学习一个合适于作为RL模块输入的表示，也值得研究。

### 端到端的学习

相对于深度学习，传统机器学习的流程往往由多个独立的模块组成，比如在一个典型的自然语言处理（Natural Language Processing）问题中，包括分词、词性标注、句法分析、语义分析等多个独立步骤，每个步骤是一个独立的任务，其结果的好坏会影响到下一步骤，从而影响整个训练的结果，这是非端到端的。

而深度学习模型在训练过程中，从输入端（输入数据）到输出端会得到一个预测结果，与真实结果相比较会得到一个误差，这个误差会在模型中的每一层传递（反向传播），每一层的表示都会根据这个误差来做调整，直到模型收敛或达到预期的效果才结束，这是端到端的。

两者相比，端到端的学习省去了在每一个独立学习任务执行之前所做的数据标注，为样本做标注的代价是昂贵的、易出错的。

### 值得研究的方向

1. 基于模型的方法。基于模型的方法不仅能大幅降低采样需求，还可以通过学习任务的动力学模型，为预测学习打下基础。

2. 提高免模型方法的数据利用率和扩展性。这是免模型学习的两处硬伤。

3. 更高效的探索策略。平衡“探索”与“利用”是RL的本质问题。除了若干经典的算法如Softmax、ϵ-Greedy、UCB和Thompson Sampling等，近期学界陆续提出了大批新算法，如Intrinsic Motivation 、Curiosity-driven Exploration、Count-based Exploration等。其实这些“新”算法的思想不少早在80年代就已出现[77]，而与DL的有机结合使它们重新得到重视。此外，OpenAI与DeepMind先后提出通过在策略参数和神经网络权重上引入噪声来提升探索策略, 开辟了一个新方向。

4. 与模仿学习(Imitation Learning ,IL)结合。IL介于RL与监督学习之间，兼具两者的优势，既能更快地得到反馈、更快地收敛，又有推理能力，很有研究价值。

5. 奖赏塑形(Reward Shaping)奖赏即反馈，其对RL算法性能的影响是巨大的。设计好的反馈信号一直是RL领域的研究热点。近年来涌现出很多基于“好奇心”的RL算法和层级RL算法，这两类算法的思路都是在模型训练的过程中插入反馈信号，从而部分地克服了反馈过于稀疏的问题。另一种思路是学习反馈函数，这是逆强化学习（Inverse RL, IRL）的主要方式之一。近些年大火的GAN也是基于这个思路来解决生成建模问题, GAN的提出者Ian Goodfellow也认为GAN就是RL的一种方式。而将GAN于传统IRL结合的GAIL已经吸引了很多学者的注意。

6. RL中的迁移学习与多任务学习。在自动驾驶领域，Gazebo、EuroTruck Simulator、TORCS、Unity、Apollo、Prescan、Panosim和Carsim等模拟器各具特色，而英特尔研究院开发的CARLA模拟器逐渐成为业界研究的标准。其他领域的模拟器开发也呈现百花齐放之势：在家庭环境模拟领域， MIT 和多伦多大学合力开发了功能丰富的VirturalHome模拟器；在无人机模拟训练领域，MIT也开发了Flight Goggles模拟器。

7. 提升RL的的泛化能力。

8.  层级RL（Hierarchical RL, HRL）

9. 与序列预测（Sequence Prediction）结合。

10. （免模型）方法探索行为的安全性（Safe RL）。一种研究思路是结合贝叶斯方法为RL代理行为的不确定性建模，从而避免过于危险的探索行为。此外，为了安全地将RL应用于现实环境中，可以在模拟器中借助混合现实技术划定危险区域，通过限制代理的活动空间约束代理的行为。

11.  关系RL。关系学习往往在训练中构建的状态链，而中间状态与最终的反馈是脱节的。RL可以将最终的反馈回传给中间状态，实现有效学习，因而成为实现关系学习的最佳方式。

12. 对抗样本RL。

13. 处理其他模态的输入。



​     

