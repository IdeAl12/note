# 强化学习的未来

## 免模型方法的缺陷

1. 免模型方法无法从不带反馈信号的样本中学习，二反馈本身是稀疏的，因此免模型方法样本利用率很低，而数据驱动的方法需要大量采样，而但凡与机械控制相关的问题，训练数据远不如视频图像的数据容易获取，因此只能在模拟器中训练。但是模拟器和现实世界之间的reality gap，直接限制了算法的泛化性能。数据的稀缺性也影响了其与DL技术的结合。
2. 免模型方法不对具体问题进行建模，而是尝试用一个通用的算法解决所有问题。基于模型的方法针对特定问题建立模型，充分利用了问题固有的信息。免模型方法在追求通用性的同时放弃这些富有价值的信息。
3. 基于模型的方法针对问题建立动力学模型，具有解释性。而免模型方法因为没有模型，解释性不强，调试困难。
4. 相比基于模型的方法，尤其是基于简单线性模型的方法，免模型方法不够稳定，训练过程中容易发散。

## 为什么DRL的工作基于免模型方法

1. 免模型的方法相对简单直观，开源实现丰富，比较容易上手。
2. 当前RL的发展还处于初级阶段，研究的重点还是集中在环境是确定的、静态的，状态主要是离散的、静态的、完全可观察的，反馈也是确定的问题。
3. 高估了DRL的能力。DQN展示出的令人兴奋的能力使得很多人围绕着DQN进行拓展，创造出了一系列同样属于免模型的工作。

## 基于模型的方法的优点

基于模型的方法一般先从数据中学习模型，然后基于学到的模型对策略进行优化。因为模型的存在，基于模型的方法可以充分利用每一个样本来逼近模型，数据利用率极大提高。此外，学到的模型汪汪队环境的变化鲁邦，当遇到新环境时，算法可以依靠已学到的模型做推理，具有很好的泛化性。

基于模型的方法还与潜力巨大的预测学习（Predictive Learning）紧密相关。

## 基于模型的方法的缺点

1. 基于模型的DRL方法相对而言不那么简单直观，RL与DL的结合方式相对更复杂，设计难度更高。目前基于模型的DRL方法通常用高斯过程、贝叶斯网络或概率神经网络（PNN）来构建模型，如Predictron模型，Probabilistic Inference for Learning COntrol (PILCO)模型或Guided Policy Search (GPS)模型。
2. 针对无法建模的问题束手无策，有些领域，比如Natural Language Processing（NLP），存在大量难以归纳成模型的任务。
3. 建模会带来误差，而且误差往往随着算法与环境的迭代交互越来越大，使得算法难以保证收敛到最优解。
4.  模型缺乏通用性，每次换一个问题，就要重新建模。

- 半模型方法：RL泰斗Rich Sutton提出的Dyna框架和其弟子David Silver提出的Dyna-2框架

## 重新审视强化学习

目前DRL在解决部分可见状态任务（如StarCraft），状态连续的任务（如机械控制任务），动态反馈任务和多代理任务中还没取得令人惊叹的突破。

